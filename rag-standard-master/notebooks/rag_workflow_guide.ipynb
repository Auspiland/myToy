{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76a7e3f",
   "metadata": {},
   "source": [
    "# RAG graph workflow guide\n",
    "\n",
    "1. naive rag workflow 설명\n",
    "2. customization\n",
    "\n",
    "# Naive RAG workflow 설명\n",
    "## 주피터 노트북 전용 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2759c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /data/aisvc_data/RAG/Rag_Standard_v2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# root 프로젝트 폴더 경로를 환경 변수에 등록\n",
    "PROJECT_ROOT = os.path.abspath('..')\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b2972",
   "metadata": {},
   "source": [
    "## `naive_rag_workflow.yaml` 설정 파일 상세 설명\n",
    "\n",
    "RAG 워크플로우는 `naive_rag_workflow.yaml` 설정 파일을 통해 다양한 구성 요소를 제어합니다. 이 섹션에서는 주요 설정 항목들에 대해 설명합니다.\n",
    "\n",
    "---\n",
    "\n",
    "```yaml\n",
    "# ---- 프로젝트 설정 ----\n",
    "project_name: \"Naive RAG 표준화 데모\"\n",
    "description: \"RAG 표준화 데모용\"\n",
    "\n",
    "# --- 그래프 선택 및 빌드 ---\n",
    "graph_settings:\n",
    "  module: \"rag_standard.graph_workflow.graph\" \n",
    "  builder_function: \"build_naive_rag_graph\"\n",
    "  # builder_args: {} # 필요시 빌더 함수에 전달할 인자\n",
    "\n",
    "# --- 토크나이저 설정 (선택된 그래프/노드에 필요한 경우) ---\n",
    "tokenizer:\n",
    "  type: \"KiwiTokenizer\" \n",
    "  included_pos_tags:\n",
    "    - NNG \n",
    "    - NNP \n",
    "    - VV  \n",
    "    - VA  \n",
    "  dictionary_path: \"\" \n",
    "\n",
    "# --- 노드 또는 파이프라인 특정 설정 ---\n",
    "graph_node_config:\n",
    "  vector_db_search: \n",
    "    index_name: \"rag_sample_project\" \n",
    "    dense_vector_field: \"text_dense\" \n",
    "    sparse_vector_field: \"text_sparse_kiwi\" \n",
    "    k: 10 \n",
    "    size: 5 \n",
    "    search_type: \"hybrid\" \n",
    "    hybrid_search_config:\n",
    "      normalization: \"min_max\" \n",
    "      combination: \"arithmetic_mean\" \n",
    "      weights: [0.3, 0.7]\n",
    "\n",
    "  generate_answer: \n",
    "    system_prompt_template: \"너는 T3Q 회사의 AI 비서이다. 사용자의 질문에 대해 문서를 참고하여 친절하고 정확하게 답변해야 한다.\"\n",
    "    user_prompt_template: |\n",
    "      ## 사용자 질문:\n",
    "      {user_query}\n",
    "\n",
    "      ## 참고 문서:\n",
    "      {documents}\n",
    "\n",
    "      ## 답변:\n",
    "    temperature: 0.7 \n",
    "    stream_output: true \n",
    "\n",
    "# --- 평가 스크립트 설정 ---\n",
    "evaluation_settings:\n",
    "  qa_dataset_path: \"data/QA/qa_dataset.parquet\" \n",
    "  results_output_dir: \"benchmark\" \n",
    "  eval_type: \"retrieval\"  \n",
    "  k_retrieval: 5 \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 프로젝트 설정 (`project_name`, `description`)\n",
    "- `project_name`: 프로젝트의 이름을 지정합니다.\n",
    "- `description`: 프로젝트에 대한 간략한 설명을 제공합니다.\n",
    "\n",
    "### 2. 그래프 선택 및 빌드 (`graph_settings`)\n",
    "- `module`: LangGraph 그래프 빌더 함수가 포함된 파이썬 모듈의 경로를 지정합니다. (예: `rag_standard.graph_workflow.graph`)\n",
    "- `builder_function`: `module` 내에서 호출할 그래프 빌더 함수의 이름을 지정합니다. (예: `build_naive_rag_graph`)\n",
    "- `builder_args` (선택 사항): `builder_function`에 전달할 인자를 딕셔너리 형태로 지정합니다.\n",
    "\n",
    "### 3. 토크나이저 설정 (`tokenizer`)\n",
    "Sparse 검색 또는 특정 노드에서 텍스트 토큰화가 필요한 경우 사용됩니다.\n",
    "- `type`: 사용할 토크나이저 클래스의 이름을 지정합니다. (현재 예시: `KiwiTokenizer`)\n",
    "- `included_pos_tags`: `KiwiTokenizer`와 같은 형태소 분석기 기반 토크나이저에서 추출할 품사 태그 목록을 지정합니다.\n",
    "- `dictionary_path`: 사용자 정의 사전 파일의 경로를 지정합니다. (프로젝트 루트 기준 상대 경로 또는 절대 경로)\n",
    "\n",
    "### 4. 노드 또는 파이프라인 특정 설정 (`graph_node_config`)\n",
    "LangGraph의 각 노드에서 `state['config']['graph_node_config']`를 통해 접근할 수 있는 설정입니다.\n",
    "\n",
    "#### 4.1. `vector_db_search` 노드 설정\n",
    "- `index_name`: OpenSearch에서 사용할 인덱스 이름을 지정합니다.\n",
    "- `dense_vector_field`: Dense 벡터가 저장된 OpenSearch 인덱스 필드명을 지정합니다.\n",
    "- `sparse_vector_field`: Sparse 벡터(토큰)가 저장된 OpenSearch 인덱스 필드명을 지정합니다.\n",
    "- `k`: KNN 검색 시 초기 검색할 문서의 수를 지정합니다.\n",
    "- `size`: 최종적으로 반환할 문서의 수를 지정합니다.\n",
    "- `search_type`: 검색 유형을 지정합니다. (`dense`, `sparse`, `hybrid` 중 선택)\n",
    "- `hybrid_search_config` (search_type이 `hybrid`일 경우):\n",
    "  - `normalization`: 하이브리드 검색 시 스코어 정규화 방식을 지정합니다. (`min_max`, `l2` 등)\n",
    "  - `combination`: 정규화된 스코어를 결합하는 방식을 지정합니다. (`arithmetic_mean`, `geometric_mean`, `harmonic_mean` 등)\n",
    "  - `weights`: Dense 검색과 Sparse 검색 결과의 가중치를 리스트 형태로 지정합니다. (예: `[0.3, 0.7]`)\n",
    "\n",
    "#### 4.2. `generate_answer` 노드 설정\n",
    "- `system_prompt_template`: LLM에 전달할 시스템 프롬프트 템플릿을 지정합니다.\n",
    "- `user_prompt_template`: LLM에 전달할 사용자 프롬프트 템플릿을 지정합니다. `{user_query}`와 `{documents}` 변수를 포함할 수 있습니다.\n",
    "- `temperature`: LLM 호출 시 사용할 `temperature` 파라미터 값을 지정합니다.\n",
    "- `stream_output`: LLM의 답변을 스트리밍 형태로 받을지 여부를 지정합니다. (`true` 또는 `false`)\n",
    "\n",
    "### 5. 평가 스크립트 설정 (`evaluation_settings`)\n",
    "`scripts/run_evaluation.py` 스크립트 실행 시 사용되는 설정입니다.\n",
    "- `qa_dataset_path`: 평가에 사용할 QA 데이터셋(Parquet 형식) 파일 경로를 지정합니다. (프로젝트 루트 기준)\n",
    "- `results_output_dir`: 평가 결과가 저장될 디렉토리 경로를 지정합니다. (프로젝트 루트 기준)\n",
    "- `eval_type`: 수행할 평가 유형을 지정합니다. (`retrieval`, `generation`, `all` 중 선택)\n",
    "- `k_retrieval`: 검색 성능 지표(Precision@k, Recall@k, F1@k) 계산 시 사용할 `k` 값을 지정합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed4247",
   "metadata": {},
   "source": [
    "## `execute_rag_pipeline()` 살펴보기\n",
    "\n",
    "`scripts/run_graph_workflow.py`에 있는 `execute_rag_pipeline`은 langgraph workflow를 실행하는 함수입니다.   \n",
    "`scripts/cli_chat`, `scripts/run_evaluation`에서도 사용됩니다.\n",
    "\n",
    "**실행 과정**\n",
    "1. `yaml config`에 정의된 `토크나이저` 결정\n",
    "2. `.env`에 정의된 opensearch host, port로 `OpensearchManager`(+하이브리드 검색 파이프라인) 결정\n",
    "3. `rag_standard/graph_workflow`에 정의된 그래프 워크플로우 import 및 실행\n",
    "\n",
    "그래프 워크플로우 및 rag pipeline 코드는 얼마든지 **커스텀**하셔서 사용하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e25019",
   "metadata": {},
   "source": [
    "```python\n",
    "def execute_rag_pipeline(\n",
    "    pipeline_config: dict, \n",
    "    user_query: str,\n",
    "    opensearch_manager_instance: OpenSearchManager = None, # 파라미터 이름 및 타입 변경\n",
    "    tokenizer_instance = None,\n",
    "    runnable_graph_instance = None \n",
    "    ) -> dict:\n",
    "    \"\"\"\n",
    "    주어진 설정과 사용자 질문으로 RAG 파이프라인을 실행하고 최종 상태를 반환합니다.\n",
    "    OpenSearch Manager, 토크나이저, 그래프를 외부에서 주입받아 재사용할 수 있습니다.\n",
    "\n",
    "    Args:\n",
    "        pipeline_config (dict): 파이프라인 설정입니다.\n",
    "        user_query (str): 사용자의 질문입니다.\n",
    "        opensearch_manager_instance (OpenSearchManager, optional): 미리 초기화된 OpenSearchManager 인스턴스입니다.\n",
    "        tokenizer_instance (object, optional): 미리 초기화된 토크나이저입니다.\n",
    "        runnable_graph_instance (Any, optional): 미리 빌드된 LangGraph 실행 가능 객체입니다.\n",
    "\n",
    "    Returns:\n",
    "        dict: 파이프라인 실행 후의 최종 상태(state)입니다. 오류 발생 시 'error' 키를 포함할 수 있습니다.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- RAG 파이프라인 실행 시작 (질문: '{user_query}') ---\")\n",
    "\n",
    "    # 1. 토크나이저 결정 (주입 또는 신규 초기화)\n",
    "    current_tokenizer = tokenizer_instance\n",
    "    if not current_tokenizer:\n",
    "        current_tokenizer = initialize_tokenizer_from_config(pipeline_config)\n",
    "    else:\n",
    "        print(\"기존 토크나이저 인스턴스를 사용합니다.\")\n",
    "        \n",
    "    # 2. OpenSearch Manager 결정 (주입 또는 신규 초기화)\n",
    "    current_opensearch_manager = opensearch_manager_instance # 변수 이름 변경\n",
    "    if not current_opensearch_manager:\n",
    "        current_opensearch_manager = initialize_opensearch_manager_with_globals() # 변수 이름 변경\n",
    "    else:\n",
    "        print(\"기존 OpenSearch Manager 인스턴스를 사용합니다.\") # 메시지 변경\n",
    "    \n",
    "    ## 하이브리드 검색일 경우) YAML에 정의된 검색 파이프라인 생성/업데이트 시도\n",
    "    vector_db_search_config = pipeline_config.get('graph_node_config', {}).get('vector_db_search', {})\n",
    "    if vector_db_search_config.get('search_type') == 'hybrid':\n",
    "        hybrid_search_config = vector_db_search_config.get('hybrid_search_config', {})\n",
    "\n",
    "        if hybrid_search_config:\n",
    "            normalization = hybrid_search_config['normalization']\n",
    "            combination = hybrid_search_config['combination']\n",
    "            weights = hybrid_search_config['weights']\n",
    "\n",
    "            hybrid_search_pipeline_name = f\"{normalization}_{combination}_{'_'.join([str(int(w*10)) for w in weights])}\"\n",
    "\n",
    "            print(f\"\\n--- '{hybrid_search_pipeline_name}' 검색 파이프라인 준비 중 (YAML 정의 사용) ---\")\n",
    "            \n",
    "            if current_opensearch_manager.check_search_pipeline(hybrid_search_pipeline_name): # 파이프라인이 존재하면 그대로 사용\n",
    "                print(f\"--- '{hybrid_search_pipeline_name}' 검색 파이프라인이 존재하므로 생성하지 않습니다. ---\")\n",
    "            else: # 파이프라인이 존재하지 않으면 생성\n",
    "                try:\n",
    "                    current_opensearch_manager.create_search_pipeline(hybrid_search_pipeline_name, normalization, combination, weights)\n",
    "                    # create_search_pipeline 내부에서 성공/실패 로그를 이미 출력합니다.\n",
    "                    print(f\"--- '{hybrid_search_pipeline_name}' 검색 파이프라인 준비 완료 ---\")\n",
    "                except Exception as e:\n",
    "                    print(f\"경고: '{hybrid_search_pipeline_name}' 검색 파이프라인 생성/업데이트 실패: {e}. 계속 진행합니다.\")\n",
    "                    # 파이프라인 생성 실패 시, 기존에 파이프라인이 존재하면 검색은 성공할 수 있음\n",
    "                    # 또는 여기서 워크플로우를 중단하도록 선택할 수도 있습니다.\n",
    "                    print(f\"--- '{hybrid_search_pipeline_name}' 검색 파이프라인 준비 실패 ---\")\n",
    "\n",
    "\n",
    "    # 3. 실행 가능한 RAG 그래프 결정 (주입 또는 신규 빌드)\n",
    "    current_runnable_graph = runnable_graph_instance\n",
    "    if not current_runnable_graph:\n",
    "        print(\"새로운 그래프 인스턴스를 빌드합니다...\")\n",
    "        try:\n",
    "            current_runnable_graph = get_graph_runnable_from_config(pipeline_config)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"그래프 빌드 실패: {str(e)}\", \n",
    "                \"llm_answer\": \"오류로 인해 답변을 생성할 수 없습니다.\"\n",
    "            }\n",
    "    else:\n",
    "        print(\"기존 그래프 인스턴스를 사용합니다.\")\n",
    "\n",
    "    # 4. 초기 상태 준비\n",
    "    initial_state = {\n",
    "        'user_query': user_query,\n",
    "        'config': pipeline_config,\n",
    "        'sparse_tokenizer': current_tokenizer, \n",
    "        'opensearch_manager': current_opensearch_manager, # 키 이름 변경: opensearch_client -> opensearch_manager\n",
    "        'llm_stream_handled_by_node': False\n",
    "    }\n",
    "    print(\"파이프라인 초기 상태가 준비되었습니다.\")\n",
    "\n",
    "    # 5. 그래프 실행\n",
    "    try:\n",
    "        print(\"LangGraph 그래프 실행 중...\")\n",
    "        final_state = current_runnable_graph.invoke(initial_state)\n",
    "        print(\"RAG 파이프라인 실행 성공.\")\n",
    "        print(\"--- RAG 파이프라인 실행 완료 ---\")\n",
    "        return final_state\n",
    "    except Exception as e:\n",
    "        print(f\"RAG 파이프라인 실행 중 오류 발생: {e}\")\n",
    "        print(\"--- RAG 파이프라인 실행 실패 ---\")\n",
    "        return {\n",
    "            \"error\": f\"파이프라인 실행 오류: {str(e)}\", \n",
    "            \"llm_answer\": f\"파이프라인 실행 중 오류로 인해 답변을 생성할 수 없습니다: {e}\"\n",
    "        }\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d39f3e4",
   "metadata": {},
   "source": [
    "## Hybrid Search Pipeline in OpenSearch\n",
    "\n",
    "Opensearch에서 하이브리드 검색은 serach pipeline이 등록되어야 사용할 수 있습니다.\n",
    "\n",
    "### 설정 (`naive_rag_workflow.yaml`)\n",
    "\n",
    "`naive_rag_workflow.yaml` 파일 내 `graph_node_config.vector_db_search` 섹션에서 하이브리드 검색 파이프라인을 설정합니다:\n",
    "\n",
    "```yaml\n",
    "graph_node_config:\n",
    "  vector_db_search:\n",
    "    index_name: \"rag_sample_project\"\n",
    "    dense_vector_field: \"text_dense\"\n",
    "    sparse_vector_field: \"text_sparse_kiwi\"\n",
    "    k: 10\n",
    "    size: 5\n",
    "    search_type: \"hybrid\" # <--- 하이브리드 검색 사용 명시\n",
    "    hybrid_search_config:\n",
    "      normalization: \"min_max\"        # 스코어 정규화 방식 (예: min_max, l2)\n",
    "      combination: \"arithmetic_mean\"  # 스코어 결합 방식 (예: arithmetic_mean, geometric_mean)\n",
    "      weights: [0.3, 0.7]             # Dense 검색과 Sparse 검색 결과의 가중치 [dense_weight, sparse_weight]\n",
    "```\n",
    "\n",
    "- `normalization`: 각 검색 결과의 스코어 범위를 통일합니다.\n",
    "- `combination`: 정규화된 스코어들을 결합하여 최종 스코어를 계산합니다.\n",
    "- `weights`: 각 검색 방식의 중요도를 조절합니다.\n",
    "\n",
    "### 파이프라인 자동 관리\n",
    "`scripts/run_graph_workflow.py`의 `execute_rag_pipeline` 함수는 워크플로우 실행 시 YAML 설정을 기반으로 OpenSearch에 해당 하이브리드 검색 파이프라인이 존재하는지 확인하고, 없으면 자동으로 생성/업데이트합니다.\n",
    "\n",
    "- **파이프라인 이름 규칙**: `hybrid_search_config`의 `normalization`, `combination`, `weights` 값을 조합하여 `f\"{normalization}_{combination}_{weight_dense*10}_{weight_sparse*10}\"` 형식의 파이프라인 이름을 사용합니다. (예: `min_max_arithmetic_mean_3_7`)\n",
    "\n",
    "- **관련 모듈**:\n",
    "  - `rag_standard/utils/opensearch_manager.py`: `check_search_pipeline`, `create_search_pipeline` 메서드를 통해 파이프라인을 관리합니다.\n",
    "  - `rag_standard/graph_workflow/node.py`: `vector_db_search` 노드에서 `execute_hybrid_search_with_pipeline`을 호출하여 하이브리드 검색을 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463a84b",
   "metadata": {},
   "source": [
    "## workflow graph 내부 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad46ee5",
   "metadata": {},
   "source": [
    "```python\n",
    "# build_naive_rag_graph 정의(rag_standard/graph_workflow/graph.py)\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from rag_standard.graph_workflow.state import NaiveRAGState\n",
    "from rag_standard.graph_workflow.node import vector_db_search, generate_answer\n",
    "\n",
    "# 참고: utils.opensearch_client.init_opensearch_client 와 utils.config_setting.load_env 는\n",
    "# 이 모듈을 직접 사용하는 대신, 실행 스크립트(예: run_graph_workflow.py)에서 관리됩니다.\n",
    "\n",
    "def build_naive_rag_graph():\n",
    "    \"\"\"\n",
    "    표준 Naive RAG 그래프를 생성하고 컴파일하는 함수입니다.\n",
    "    이 함수는 설정 파일의 'graph_settings'에 따라 동적으로 호출될 수 있습니다.\n",
    "    \"\"\"\n",
    "    graph = StateGraph(NaiveRAGState)\n",
    "\n",
    "    # 그래프에 노드(Node) 추가\n",
    "    graph.add_node('vector_db_search', vector_db_search) # 벡터 DB 검색 노드\n",
    "    graph.add_node('generate_answer', generate_answer)   # 답변 생성 노드\n",
    "\n",
    "    # 그래프에 엣지(Edge) 추가 (노드 간의 흐름 정의)\n",
    "    graph.add_edge(START, 'vector_db_search')             # 시작점에서 vector_db_search 노드로 이동\n",
    "    graph.add_edge('vector_db_search', 'generate_answer') # vector_db_search 노드에서 generate_answer 노드로 이동\n",
    "    graph.add_edge('generate_answer', END)                # generate_answer 노드에서 종료점으로 이동\n",
    "\n",
    "    # 그래프 컴파일\n",
    "    compiled_graph = graph.compile()\n",
    "    print(\"Naive RAG 그래프가 빌드 및 컴파일되었습니다.\")\n",
    "    return compiled_graph\n",
    "\n",
    "# 만약 다른 종류의 그래프 빌더 함수가 있다면 여기에 추가합니다.\n",
    "# 예: def build_advanced_rag_graph(custom_param1: str): ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff733e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive RAG 그래프가 빌드 및 컴파일되었습니다.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7f2d1ddd9640>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rag_standard.graph_workflow.graph import build_naive_rag_graph\n",
    "\n",
    "naive_rag_graph = build_naive_rag_graph()\n",
    "naive_rag_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2043b9",
   "metadata": {},
   "source": [
    "naive graph는 두가지 노드가 존재합니다.\n",
    "1. `vector_db_search`: 벡터 디비 검색\n",
    "2. `generate_answer`: 검색된 문서와 사용자 질문으로 프롬프트 구성 후 LLM으로 답변 생성\n",
    "\n",
    "노드 내부도 프로젝트에 따라 **커스텀**하시면 됩니다!\n",
    "\n",
    "그래프 상태는 다음과 같이 정의되어 있습니다. (`rag_standard/graph_workflow/state.py`)\n",
    "- `config`: `naive_rag_workflow.yaml` 파일\n",
    "- `user_query`: 사용자 질문\n",
    "- `sparse_tokenizer`: `KiwiTokenizer`같은 sparse 검색용 토크나이저\n",
    "- `opensearch_manager`: `rag_standard/utils/opensearch_manager.py`에 정의된 오픈서치 매니저\n",
    "- `retrieved_documents`: 오픈서치 검색 결과\n",
    "- `llm_answer`: LLM 답변\n",
    "- `llm_stream_handled_by_node`: 스트리밍으로 답변했는지 아닌지 여부\n",
    "  - 스트리밍 답변이 아니면 원래 `script/` 하위에 있는 파이썬 파일들에서 처리하지만, 스트리밍 답변이면 `generate_answer` 노드에서 바로 처리리\n",
    "\n",
    "`sparse_tokenizer`와 `opensearch_manager` 타입이 `Any`인 이유: 그래프 출력해서 mermaid_png로 보고싶을 때 원래 타입 쓰면 에러 발생...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f11ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "# from rag_standard.utils.sparse_tokenizer import SparseTokenizer\n",
    "# from rag_standard.utils.opensearch_manager import OpenSearchManager\n",
    "\n",
    "# Naive RAG State 정의\n",
    "class NaiveRAGState(TypedDict):\n",
    "  config: Dict # 파이프라인 설정\n",
    "  user_query: str # 사용자 질문\n",
    "  sparse_tokenizer: Any # SparseTokenizer\n",
    "  opensearch_manager: Any # OpenSearchManager\n",
    "  retrieved_documents: List[dict] # 검색 결과\n",
    "  llm_answer: str # LLM 답변\n",
    "  # 스트리밍이 노드 레벨에서 처리되었는지 여부를 나타내는 플래그 (선택 사항)\n",
    "  llm_stream_handled_by_node: Optional[bool] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a1805",
   "metadata": {},
   "source": [
    "NaiveRAGState 구성을 바꾸거나   \n",
    "새로운 State를 해당 파이썬 파일에 추가해나가시면 됩니다!\n",
    "\n",
    "노드는 다음과 같이 정의되어 있습니다 (`rag_standard/graph_workflow/node.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_standard/graph_workflow/node.py\n",
    "from rag_standard.utils.model_call import embed_text, generate_llm_response\n",
    "from rag_standard.graph_workflow.state import NaiveRAGState\n",
    "# from rag_standard.utils.opensearch_manager import OpenSearchManager # 타입 힌팅을 위해 필요할 수 있음\n",
    "import sys # 스트리밍 출력을 위해 추가\n",
    "\n",
    "def vector_db_search(state: NaiveRAGState) -> NaiveRAGState:\n",
    "    \"\"\"\n",
    "    벡터 데이터베이스 검색을 수행하고 결과를 state['retrieved_documents']에 저장합니다.\n",
    "    state['opensearch_manager']를 사용하여 검색 로직을 실행합니다.\n",
    "    state['config']['graph_node_config']['vector_db_search']['search_type']에 따라 검색 유형이 결정됩니다.\n",
    "    \"\"\"\n",
    "    print('='*30)\n",
    "    print(\"노드 실행: vector_db_search\")\n",
    "    search_cfg = state['config']['graph_node_config']['vector_db_search']\n",
    "    opensearch_manager = state['opensearch_manager'] # OpenSearchManager 인스턴스를 가져옵니다.\n",
    "\n",
    "    index_name = search_cfg['index_name']\n",
    "    size = search_cfg['size']\n",
    "    # _source 제외 필드 설정 (dense와 sparse 필드 모두 기본적으로 제외 시도)\n",
    "    source_excludes = [search_cfg.get('dense_vector_field'), search_cfg.get('sparse_vector_field')]\n",
    "    source_excludes = [f for f in source_excludes if f is not None] # None 값 제거\n",
    "\n",
    "    search_type = search_cfg['search_type']\n",
    "    response = None\n",
    "\n",
    "    if search_type == 'dense':\n",
    "        query_embedding = embed_text([state['user_query']])[0]\n",
    "        response = opensearch_manager.execute_dense_search(\n",
    "            index_name=index_name,\n",
    "            query_vector=query_embedding,\n",
    "            dense_field=search_cfg['dense_vector_field'],\n",
    "            k=search_cfg['k'],\n",
    "            size=size,\n",
    "            source_excludes=source_excludes\n",
    "        )\n",
    "    elif search_type == 'sparse':\n",
    "        tokenized_query = state['sparse_tokenizer'].tokenize(state['user_query'])\n",
    "        response = opensearch_manager.execute_sparse_search(\n",
    "            index_name=index_name,\n",
    "            tokenized_query=tokenized_query,\n",
    "            sparse_field=search_cfg['sparse_vector_field'],\n",
    "            size=size,\n",
    "            source_excludes=source_excludes\n",
    "        )\n",
    "    elif search_type == 'hybrid':\n",
    "        tokenized_query = state['sparse_tokenizer'].tokenize(state['user_query'])\n",
    "        query_embedding = embed_text([state['user_query']])[0]\n",
    "\n",
    "        hybrid_search_config = search_cfg['hybrid_search_config']\n",
    "        normalization = hybrid_search_config['normalization']\n",
    "        combination = hybrid_search_config['combination']\n",
    "        weights = hybrid_search_config['weights']\n",
    "        hybrid_search_pipeline_name = f\"{normalization}_{combination}_{'_'.join([str(int(w*10)) for w in weights])}\"\n",
    "        \n",
    "        response = opensearch_manager.execute_hybrid_search_with_pipeline(\n",
    "            index_name=index_name,\n",
    "            pipeline_name=hybrid_search_pipeline_name,\n",
    "            query_vector=query_embedding,\n",
    "            dense_field=search_cfg['dense_vector_field'],\n",
    "            k_dense=search_cfg['k'], # 'k'는 dense 검색의 k로 사용\n",
    "            tokenized_query=tokenized_query,\n",
    "            sparse_field=search_cfg['sparse_vector_field'],\n",
    "            size=size,\n",
    "            source_excludes=source_excludes\n",
    "        )\n",
    "    else:\n",
    "        # opensearch_manager의 로그와는 별개로, 노드 레벨에서도 오류를 명확히 발생시킵니다.\n",
    "        raise ValueError(f\"지원되지 않는 검색 유형입니다: {search_type}. 'dense', 'sparse', 'hybrid' 중 하나여야 합니다.\")\n",
    "\n",
    "    state['retrieved_documents'] = response['hits']['hits']\n",
    "    print(f\"vector_db_search: {len(state['retrieved_documents'])}개의 문서를 검색했습니다.\")\n",
    "    return state\n",
    "\n",
    "def generate_answer(state: NaiveRAGState) -> NaiveRAGState:\n",
    "    \"\"\"\n",
    "    검색된 문서를 사용하여 LLM으로부터 답변을 생성합니다.\n",
    "    결과는 state['llm_answer']에 저장되며, 설정에 따라 스트리밍 출력을 합니다.\n",
    "    \"\"\"\n",
    "    print('='*30)\n",
    "    print(\"노드 실행: generate_answer\")\n",
    "    llm_cfg = state['config']['graph_node_config']['generate_answer']\n",
    "    \n",
    "    # 검색된 문서가 없는 경우 처리\n",
    "    if not state.get('retrieved_documents'):\n",
    "        print(\"generate_answer: 검색된 문서가 없어 답변 생성을 건너뜁니다.\")\n",
    "        state['llm_answer'] = llm_cfg.get('no_documents_found_answer', \"죄송합니다, 관련된 정보를 찾지 못했습니다.\")\n",
    "        state['llm_stream_handled_by_node'] = False # 스트리밍을 하지 않았으므로 False\n",
    "        return state\n",
    "\n",
    "    # 문서 포맷팅\n",
    "    documents_formatted = []\n",
    "    for doc in state['retrieved_documents']:\n",
    "        source = doc.get('_source', {})\n",
    "        file_name = source.get('file_name', '알 수 없는 파일')\n",
    "        # retrieve_content 대신 text_content 또는 다른 필드 이름을 사용할 수 있으므로 설정에서 가져오도록 고려 가능\n",
    "        content = source.get(llm_cfg.get('document_content_field', 'retrieve_content'), '내용 없음')\n",
    "        documents_formatted.append(f'# {file_name}\\n\\n{content}')\n",
    "    \n",
    "    documents_string = \"\\n\\n---\\n\\n\".join(documents_formatted)\n",
    "\n",
    "    system_prompt = llm_cfg.get('system_prompt_template', \"너는 T3Q 회사의 AI 비서야. 사용자의 질문에 대해 친절하고 정확하게 답변해야 해.\")\n",
    "    user_prompt_template = llm_cfg.get('user_prompt_template', \"\"\"다음은 사용자의 질문과 관련된 문서들이야. 이 문서들을 참고해서 사용자의 질문에 답변해줘.\n",
    "    문서에 명시적으로 언급되지 않은 내용은 답변에 포함하지 마.\n",
    "\n",
    "    [사용자 질문]\n",
    "    {user_query}\n",
    "\n",
    "    [참고 문서]\n",
    "    {documents}\"\"\")\n",
    "    \n",
    "    user_prompt = user_prompt_template.format(user_query=state['user_query'], documents=documents_string)\n",
    "\n",
    "    should_stream_output = llm_cfg.get('stream_output', False)\n",
    "    accumulated_response_strings = []\n",
    "\n",
    "    llm_params = {\n",
    "        \"temperature\": llm_cfg.get('temperature', 0.7),\n",
    "        \"top_p\": llm_cfg.get('top_p', 0.8),\n",
    "        \"top_k\": llm_cfg.get('top_k', 20),\n",
    "        \"repetition_penalty\": llm_cfg.get('repetition_penalty', 1.05)\n",
    "    }\n",
    "\n",
    "    print(\"=\"*15, f\"generate_answer: LLM 호출 시작 (스트리밍: {should_stream_output})\", \"=\"*15)\n",
    "    if should_stream_output:\n",
    "        response_generator = generate_llm_response(\n",
    "            system_prompt=system_prompt, \n",
    "            user_prompt=user_prompt, \n",
    "            stream=True, \n",
    "            **llm_params\n",
    "        )\n",
    "        # 스트리밍 응답 처리: generate_llm_response가 다양한 LLM SDK의 청크를 반환할 수 있으므로,\n",
    "        # 아래 로직은 일반적인 경우를 다루려고 시도합니다.\n",
    "        # model_call.py의 generate_llm_response가 반환하는 청크의 정확한 구조에 맞춰 조정하는 것이 가장 좋습니다.\n",
    "        for chunk_obj in response_generator:\n",
    "            content_piece = None\n",
    "            if hasattr(chunk_obj, 'choices') and chunk_obj.choices and \\\n",
    "               hasattr(chunk_obj.choices[0], 'delta') and \\\n",
    "               hasattr(chunk_obj.choices[0].delta, 'content'):\n",
    "                content_piece = chunk_obj.choices[0].delta.content # OpenAI 유사 API\n",
    "            elif hasattr(chunk_obj, 'text'): # 일부 API는 chunk.text로 제공\n",
    "                content_piece = chunk_obj.text\n",
    "            elif isinstance(chunk_obj, str): # 이미 문자열로 반환되는 경우\n",
    "                content_piece = chunk_obj\n",
    "            # 여기에 다른 LLM SDK의 청크 구조에 대한 elif 조건들을 추가할 수 있습니다.\n",
    "            \n",
    "            if content_piece:\n",
    "                print(content_piece, end='', flush=True) \n",
    "                accumulated_response_strings.append(content_piece)\n",
    "            elif chunk_obj is None and not accumulated_response_strings:\n",
    "                pass # 일부 모델의 초기 빈 청크는 무시\n",
    "\n",
    "        print() # 스트리밍 완료 후 줄바꿈\n",
    "        state['llm_answer'] = \"\".join(accumulated_response_strings)\n",
    "        state['llm_stream_handled_by_node'] = True\n",
    "    else:\n",
    "        full_response = generate_llm_response(\n",
    "            system_prompt=system_prompt, \n",
    "            user_prompt=user_prompt, \n",
    "            stream=False, \n",
    "            **llm_params\n",
    "        )\n",
    "        state['llm_answer'] = full_response\n",
    "        state['llm_stream_handled_by_node'] = False\n",
    "    \n",
    "    print(\"=\"*15, \"generate_answer: LLM 답변 생성 완료.\", \"=\"*15)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566a38f",
   "metadata": {},
   "source": [
    "그래프 직접 실행해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ecb08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rag_standard.utils.config_setting import load_yaml_config, load_env\n",
    "from rag_standard.utils.sparse_tokenizer import KiwiTokenizer\n",
    "from rag_standard.utils.opensearch_manager import OpenSearchManager\n",
    "\n",
    "load_env()\n",
    "\n",
    "user_query = 'CTP CTQ가 뭐야?'\n",
    "pipeline_config = load_yaml_config(os.path.abspath('../configs/naive_rag_workflow.yaml'))\n",
    "sparse_tokenizer = KiwiTokenizer(included_pos_tags=pipeline_config['tokenizer']['included_pos_tags'])\n",
    "\n",
    "host = os.getenv(\"OPENSEARCH_HOST\")\n",
    "port = os.getenv(\"OPENSEARCH_PORT\")\n",
    "opensearch_manager = OpenSearchManager(host=host, port=port)\n",
    "\n",
    "initial_state = {\n",
    "        'user_query': user_query,\n",
    "        'config': pipeline_config,\n",
    "        'sparse_tokenizer': sparse_tokenizer, \n",
    "        'opensearch_manager': opensearch_manager, # 키 이름 변경: opensearch_client -> opensearch_manager\n",
    "        'llm_stream_handled_by_node': True\n",
    "}\n",
    "naive_rag_graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d905df",
   "metadata": {},
   "source": [
    "(랭그래프 문법 참조 후)   \n",
    "노드 내부 구현을 바꾸시거나   \n",
    "새로운 노드들을 추가해나가면서   \n",
    "그래프 워크플로우를 개선해나가면 됩니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a099f",
   "metadata": {},
   "source": [
    "# RAG 파이프라인 평가 (`scripts/run_evaluation.py`)\n",
    "\n",
    "`scripts/run_evaluation.py` 스크립트를 통해 RAG 파이프라인의 검색(Retrieval) 및 생성(Generation) 성능을 체계적으로 평가할 수 있습니다!   \n",
    "하지만 생성보다는 검색 평가가 더 효과가 크기 때문에 검색 평가에 집중합니다.\n",
    "\n",
    "## 평가 설정 (`naive_rag_workflow.yaml`)\n",
    "\n",
    "`naive_rag_workflow.yaml` 파일의 `evaluation_settings` 섹션에서 평가 관련 설정을 관리합니다:\n",
    "```yaml\n",
    "evaluation_settings:\n",
    "  qa_dataset_path: \"data/QA/qa_dataset.parquet\" # QA 데이터셋 Parquet 파일 경로 (프로젝트 루트 기준)\n",
    "  results_output_dir: \"benchmark\" # 평가 결과 저장 디렉토리 (프로젝트 루트 기준)\n",
    "  eval_type: \"retrieval\"  # 수행할 평가 유형: 'retrieval', 'generation', 또는 'all'\n",
    "  k_retrieval: 5 # 검색 성능 지표(P@k, R@k, F1@k) 계산 시 사용할 k값\n",
    "```\n",
    "\n",
    "## QA 데이터셋 형식\n",
    "평가에는 **Parquet** 형식의 QA 데이터셋이 사용됩니다.\n",
    "\n",
    "- 필수 컬럼:\n",
    "  - `query_id` `(str, optional)`: 각 질문의 고유 ID. 지정하지 않으면 순번으로 자동 생성됩니다.\n",
    "  - `query` `(str)`: 사용자 질문.\n",
    "  - `ground_truth_retrieved_doc_ids` `(List[str] 또는 Set[str])`: 정답 문서 ID 목록 (검색 평가용). `run_evaluation.py`에서 문자열 세트(Set[str])로 변환하여 사용합니다.\n",
    "  - `ground_truth_answers` `(List[str])`: 정답 답변 목록 (생성 평가용). `run_evaluation.py`에서 문자열 리스트(List[str])로 변환하여 사용합니다.\n",
    "\n",
    "- 데이터셋 경로: `qa_dataset_path`에 지정된 경로는 프로젝트 루트를 기준으로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시\n",
    "import pandas as pd\n",
    "\n",
    "test_dataset = [\n",
    "    {\n",
    "        \"id\": \"qa_1\",\n",
    "        \"query\": \"국방혁신 4.0 기본계획의 주요 내용은 무엇인가요?\",\n",
    "        \"ground_truth_retrieved_doc_ids\": {\"01954ff18c6530a9bccc4539c75109da\"},\n",
    "        \"ground_truth_answers\": \"국방혁신 4.0 기본계획은 북핵·미사일 대응능력 강화, 군사전략 발전, AI 기반 첨단전력 확보, 군구조 및 교육훈련 혁신, 국방 R&D 및 전력증강체계 재설계 등 5대 중점을 가지고 있습니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"qa_2\",\n",
    "        \"query\": \"국방 AI 센터 추진팀의 설치 목적은 무엇인가요?\",\n",
    "        \"ground_truth_retrieved_doc_ids\": {\"3305a14be1c0794dfda47eaa79757208\"},\n",
    "        \"ground_truth_answers\": \"국방AI센터 창설과 관련한 소관 사무에 대하여 책임있고 신속하게 대응하고, 그 성과 창출을 뒷받침하기 위함입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"qa_3\", # 관련 없는 문서 ID 및 답변 예시\n",
    "        \"query\": \"오늘 날씨는 어떤가요?\",\n",
    "        \"ground_truth_retrieved_doc_ids\": {\"non_existent_id_1\", \"non_existent_id_2\"},\n",
    "        \"ground_truth_answers\": \"죄송합니다, 저는 날씨 정보를 제공할 수 없습니다.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(test_dataset)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dfcdd7",
   "metadata": {},
   "source": [
    "## 평가 실행\n",
    "다음 명령어를 사용하여 평가를 실행합니다:\n",
    "```bash\n",
    "python scripts/run_evaluation.py configs/naive_rag_workflow.yaml\n",
    "```\n",
    "스크립트는 지정된 QA 데이터셋을 사용하여 각 질문에 대해 RAG 파이프라인을 실행하고, 설정된 `eval_type`에 따라 검색 및/또는 생성 성능 지표를 계산합니다.\n",
    "\n",
    "## 평가 지표\n",
    "`rag_standard/utils/evaluation_metrics.py`에 정의된 함수들을 사용하여 다음 지표들을 계산합니다:\n",
    "\n",
    "- **검색 지표 (Retrieval Metrics)**:\n",
    "  - **Precision@k**: 상위 k개 검색 결과 중 정답 문서의 비율.\n",
    "  - **Recall@k**: 전체 정답 문서 중 상위 k개 검색 결과에 포함된 비율.\n",
    "  - **F1@k**: Precision@k와 Recall@k의 조화 평균.\n",
    "- `k` 값은 evaluation_settings.k_retrieval에서 설정합니다.\n",
    "\n",
    "- **생성 지표 (Generation Metrics)**:\n",
    "  - **ROUGE-1/2/L (F1-score)**: 생성된 답변과 정답 답변 간의 n-gram 중복 기반 유사도.\n",
    "  - **BLEU**: 생성된 답변과 정답 답변 간의 n-gram 정밀도 기반 유사도 (NLTK 라이브러리 사용).\n",
    "\n",
    "## 결과 출력\n",
    "평가 결과는 `evaluation_settings.results_output_dir`에 지정된 디렉토리 (예: `benchmark/`) 하위에 각 실행별 고유 시도 번호로 생성된 폴더(예: 0, 1, 2, ...) 내에 저장됩니다.\n",
    "\n",
    "- `detailed_evaluation_results.csv`: 각 QA 항목별 상세 평가 지표가 포함된 CSV 파일.\n",
    "- `summary_metrics.csv`: 전체 QA 데이터셋에 대한 평균 평가 지표 요약 CSV 파일.\n",
    "\n",
    "사용한 config_file의 복사본: 평가 실행 시 사용된 YAML 설정 파일의 복사본이 함께 저장되어 실험의 재현성을 높입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36f145",
   "metadata": {},
   "source": [
    "# customization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda2ee8",
   "metadata": {},
   "source": [
    "## 공통부분 (utils) 커스텀\n",
    "\n",
    "`rag_standard/utils/` 디렉토리에는 RAG 워크플로우 전반에서 사용되는 유틸리티 함수 및 클래스들이 포함되어 있습니다.   \n",
    "프로젝트의 특정 요구사항에 맞춰 이 부분들을 커스텀할 수 있습니다.\n",
    "\n",
    "### 주요 커스텀 대상 파일\n",
    "\n",
    "- **`config_setting.py`**: \n",
    "  - `.env` 파일 로드 방식이나 YAML 설정 파일 처리 로직을 변경할 수 있습니다.\n",
    "  - 예를 들어, 다른 설정 파일 형식을 지원하거나, 환경 변수 관리 정책을 변경할 때 수정합니다.\n",
    "\n",
    "- **`evaluation_metrics.py`**:\n",
    "  - 새로운 평가 지표를 추가하거나 기존 지표(Precision@k, Recall@k, ROUGE, BLEU 등)의 계산 방식을 수정할 수 있습니다.\n",
    "  - 프로젝트 특화된 평가 기준이 있다면 여기에 구현합니다.\n",
    "\n",
    "- **`model_call.py`**:\n",
    "  - 사용하는 LLM 또는 임베딩 모델을 변경하거나, API 호출 방식(URL, 파라미터 등)을 수정할 수 있습니다.\n",
    "  - 다른 모델 제공자를 사용하거나, 모델 호출 시 추가적인 로직(예: 재시도, 로깅)을 넣고 싶을 때 커스텀합니다.\n",
    "\n",
    "- **`opensearch_manager.py`**:\n",
    "  - OpenSearch 연결 방식, 인덱스 관리, 검색 쿼리 실행 로직 등을 수정할 수 있습니다.\n",
    "  - OpenSearch와의 상호작용을 더 세밀하게 제어하고 싶을 때 커스텀합니다.\n",
    "    - 예를 들어, `create_search_pipeline` 메서드를 수정하여 다양한 파이프라인 정의를 지원할 수 있습니다.\n",
    "\n",
    "- **`setup_logger.py`**:\n",
    "  - 로깅 포맷, 레벨, 핸들러(파일, 콘솔 등) 설정을 변경할 수 있습니다.\n",
    "\n",
    "- **`sparse_tokenizer.py`**:\n",
    "  - `KiwiTokenizer` 외의 다른 한국어 형태소 분석기나 Sparse 토큰화 방식을 사용하고 싶을 때, `SparseTokenizer` 추상 클래스를 상속받아 새로운 토크나이저를 구현할 수 있습니다.\n",
    "  - 기존 `KiwiTokenizer`의 품사 태그 선택 기준이나 사용자 사전 로드 방식을 변경할 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efae4c4",
   "metadata": {},
   "source": [
    "### 주의사항\n",
    "`utils` 디렉토리의 코드는 여러 스크립트와 그래프 노드에서 공유되므로, 수정 시에는 전체 워크플로우에 미치는 영향을 고려해야 합니다.   \n",
    "팀원들과 충분히 상의 후 진행하는 것이 좋습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef921d",
   "metadata": {},
   "source": [
    "## RAG workflow yaml 설정 파일 커스텀\n",
    "\n",
    "`configs/naive_rag_workflow.yaml` 파일은 RAG 파이프라인의 동작을 상세하게 제어하는 핵심 설정 파일입니다.   \n",
    "이미 앞선 섹션에서 각 항목에 대한 설명을 다루었지만, 여기서는 커스텀 관점에서 몇 가지 추가적인 가이드를 제공합니다.\n",
    "\n",
    "### 1. 새로운 노드 설정 추가\n",
    "만약 `rag_standard/graph_workflow/graph.py`에서 새로운 노드를 정의하고 그래프에 추가했다면, 해당 노드에 필요한 설정을 `graph_node_config` 아래에 추가할 수 있습니다.\n",
    "예를 들어, `query_rewriter`라는 노드를 추가하고, 이 노드가 특정 LLM 모델과 프롬프트를 사용하도록 설정하고 싶다면 다음과 같이 추가할 수 있습니다:\n",
    "```yaml\n",
    "graph_node_config:\n",
    "  # ... 기존 vector_db_search, generate_answer 설정 ...\n",
    "  query_rewriter:\n",
    "    llm_model_name: \"qwen-3\"\n",
    "    rewrite_prompt_template: \"다음 사용자 질문을 검색에 더 적합하도록 명확하게 재작성해주세요: {user_query}\"\n",
    "    temperature: 0.2\n",
    "```\n",
    "이렇게 추가된 설정은 해당 노드 내에서 `state['config']['graph_node_config']['query_rewriter']`와 같이 접근하여 사용할 수 있습니다.\n",
    "\n",
    "### 2. 기존 노드 설정 확장\n",
    "기존 노드(`vector_db_search`, `generate_answer`)의 동작을 변경하기 위해 설정을 추가하거나 수정할 수 있습니다.\n",
    "- `vector_db_search`:\n",
    "  - `search_type`에 따라 `dense`, `sparse`, `hybrid` 외에 새로운 검색 전략을 추가하고 관련 설정을 정의할 수 있습니다 (예: `custom_search_config`).\n",
    "  - OpenSearch 외 다른 벡터 DB를 사용한다면, 해당 DB에 맞는 연결 정보나 파라미터를 추가할 수 있습니다.\n",
    "- `generate_answer`:\n",
    "  - `temperature`, `stream_output` 외에 LLM 호출에 필요한 다른 파라미터(예: `top_p`, `max_tokens`)를 추가하고, `rag_standard/utils/model_call.py`의 `generate_llm_response` 함수나 `rag_standard/graph_workflow/node.py`의 `generate_answer` 노드에서 이를 사용하도록 수정할 수 있습니다.\n",
    "  - 문서 포맷팅 방식이나, 검색 결과가 없을 때의 기본 답변(`no_documents_found_answer`) 등을 변경할 수 있습니다.\n",
    "\n",
    "### 3. 그래프 빌더 변경\n",
    "`graph_settings`의 `module`과 `builder_function`을 수정하여 완전히 다른 구조의 RAG 그래프를 사용하도록 설정할 수 있습니다. 예를 들어, `rag_standard.graph_workflow.graph` 모듈 내에 `build_advanced_rag_graph`라는 새로운 그래프 빌더 함수를 만들었다면 다음과 같이 변경합니다:\n",
    "```yaml\n",
    "graph_settings:\n",
    "  module: \"rag_standard.graph_workflow.graph\"\n",
    "  builder_function: \"build_advanced_rag_graph\"\n",
    "  # builder_args: { \"custom_param\": \"value\" } # 필요시 새 빌더 함수에 인자 전달\n",
    "```\n",
    "\n",
    "### 4. 토크나이저 및 평가 설정 커스텀\n",
    "- `tokenizer`: 다른 형태소 분석기나 토큰화 라이브러리를 사용하기 위해 `type`을 변경하고, 해당 토크나이저에 필요한 설정을 추가할 수 있습니다. (예: `dictionary_path` 외의 다른 설정)\n",
    "- `evaluation_settings`: 평가 데이터셋의 형식이나, 평가 결과 저장 방식, 추가적인 평가 지표 관련 설정을 추가할 수 있습니다.\n",
    "\n",
    "**핵심 아이디어**: `naive_rag_workflow.yaml` 파일은 단순히 정해진 키만 사용하는 것이 아니라, 여러분의 RAG 워크플로우 로직(파이썬 코드)에서 참조할 수 있는 모든 종류의 설정을 담는 유연한 저장소로 활용될 수 있습니다. 워크플로우를 커스텀하면서 필요한 설정이 있다면 이 파일에 적절한 구조로 추가하고, 코드에서 해당 설정을 읽어 사용하면 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
